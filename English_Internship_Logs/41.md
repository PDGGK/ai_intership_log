# Technical Learning Log

## 2024-01-12 Technical Learning Log

<div align="center">
  <img src="https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white"/>
  <img src="https://img.shields.io/badge/FastAPI-009688?style=for-the-badge&logo=fastapi&logoColor=white"/>
  <img src="https://img.shields.io/badge/LangChain-00A0DC?style=for-the-badge&logo=chainlink&logoColor=white"/>
  <img src="https://img.shields.io/badge/OpenAI-412991?style=for-the-badge&logo=openai&logoColor=white"/>
</div>

# Building a Reliable LLM Tool Calling System: Learning from Practice

<div align="center">
  <p><strong>Ensuring Accurate Tool Execution in LLM-Powered Applications</strong></p>
  <p>🔧 Tool Calling Framework | 🤖 LLM Control | 🔍 Verification Systems | ⚙️ Robust Engineering</p>
</div>

---

## 📑 Table of Contents

- [📘 Introduction](#introduction-when-ai-meets-engineering-practice-)
- [🔍 Problem Discovery](#problem-discovery-when-llms-are-too-smart-)
- [🧠 In-Depth Analysis](#in-depth-analysis-why-does-this-happen-)
- [⚙️ Solution](#solution-building-a-reliable-tool-calling-framework-)
- [📝 Prompt Engineering](#prompt-engineering-guiding-llms-to-use-tools-correctly-)
- [💡 Practical Experience](#practical-experience-and-technical-insights-)
- [🚀 Future Optimization](#future-optimization-directions-)
- [📌 Conclusion](#conclusion-)

---

## Introduction: When AI Meets Engineering Practice <img src="https://img.shields.io/badge/Introduction-FF6B6B?style=flat-square&logo=bookstack&logoColor=white"/>

In today's rapidly developing world of artificial intelligence, large language models (LLMs) have become the core of many intelligent systems. However, when applying LLMs to actual engineering projects, we often encounter unexpected challenges. Today, I want to share an interesting problem encountered during my internship: how to ensure that LLMs actually execute tool calls rather than "cleverly" generating results on their own.

Let me use the development experience of a health food advertisement review system as an example to explore this issue in depth.

---

## Problem Discovery: When LLMs are "Too Smart" <img src="https://img.shields.io/badge/Problem-FFA000?style=flat-square&logo=problemsolver&logoColor=white"/>

### Problem Scenario 🔍
Imagine this scenario: we developed a system to check whether health food logos are present in advertisement images. Theoretically, the process should be:
1. 🤖 LLM analyzes the task requirements
2. 🛠️ Calls the logo detection tool
3. ✨ Makes judgments based on actual detection results

### Code Example
```python
# Original simple implementation
async def process_check(self, image_file: UploadFile) -> Dict[str, Any]:
    prompt = """
    Please analyze whether the image contains a health food logo.
    You can use the check_logo_presence tool to detect the logo.
    """
    
    response = await self.llm.chat(prompt)
    
    # Problem: LLM might directly generate an answer containing "Observation",
    # skipping the actual tool call
    if "Action:" in response:
        # This might never execute
        result = await self.execute_tool(...)
    
    return self.parse_result(response)
```

---

## In-Depth Analysis: Why Does This Happen? <img src="https://img.shields.io/badge/Analysis-4CAF50?style=flat-square&logo=analytics&logoColor=white"/>

### LLM Behavior Analysis 🧠
To understand this problem, we need to understand how LLMs work: they are essentially models that make predictions based on context.

### Typical Pattern
```
Action: check_logo_presence
Action Input: {"image_data": "..."}
Observation: {"status": "success", "has_logo": true, ...}
```

---

## Solution: Building a Reliable Tool Calling Framework <img src="https://img.shields.io/badge/Solution-2196F3?style=flat-square&logo=solution&logoColor=white"/>

### Core Implementation ⚙️
```python
class LLMController:
    def __init__(self):
        # Conversation history and state management
        self.conversation_parts = []
        self.current_tool_result = None
        self.has_logo_check = False  # Tool call status flag
        
        # Configuration parameters
        self.max_steps = 10
        self.early_stop_threshold = 2
        
    async def process(self, text_content: str, image_info: Dict[str, Any]) -> BaseResponse:
        # Build initial prompt
        conversation = self._build_prompt(text_content)
        step_count = 0
        no_tool_call_count = 0
        
        while step_count < self.max_steps:
            # 1. Get LLM response
            llm_response = await llm_chat(conversation)
            
            # 2. Parse tool call
            tool_name, tool_args, response_text = self._parse_latest_plugin_call(llm_response)
            
            # 3. Execute tool call and add observation results
            if tool_name:
                no_tool_call_count = 0
                tool_result = await self._execute_tool(tool_name, tool_args)
                conversation += f"\nObservation: {tool_result}\nThought:"
            else:
                no_tool_call_count += 1
            
            # 4. Check for final answer
            if self._is_final_answer(llm_response):
                return self._validate_and_parse_result(llm_response)
                
            step_count += 1
```

### Key Improvements 🚀
1. ✅ Strict state management
2. 🔄 Clear tool calling process
3. 🛡️ Complete result validation

---

## Prompt Engineering: Guiding LLMs to Use Tools Correctly <img src="https://img.shields.io/badge/Prompting-9C27B0?style=flat-square&logo=openai&logoColor=white"/>

### Prompt Template 📝
```python
def get_analysis_prompt(self) -> str:
    return """Please analyze whether the advertisement contains a health food logo.

Important rules:
1. You MUST use the check_logo_presence tool for detection
2. Do NOT make judgments or guess results on your own
3. You MUST wait for the tool to return actual results
4. You MUST accurately reference tool return values in your final answer

Format to use:
Question: Input question
Thought: Think about the next action
Action: Tool name to use
Action Input: Tool parameters
Observation: Results returned by the tool
... (above steps can be repeated)
Thought: Now I know the answer
Final Answer: Final answer (JSON format)
"""
```

---

## Practical Experience and Technical Insights <img src="https://img.shields.io/badge/Experience-00BCD4?style=flat-square&logo=expertise&logoColor=white"/>

### Core Principles 💡
1. 🎯 **Don't overly trust LLM's automatic behavior**
   - LLM's predictive behavior needs strict control
   - Each tool call must be verified
   - Maintain system state consistency

2. 📊 **Importance of logging systems**
   - Record each key step
   - Include sufficient context information
   - Helps with problem diagnosis

3. 🔄 **Value of state management**
   - Clear state transitions
   - Complete error handling
   - Traceable execution flow

---

## Future Optimization Directions <img src="https://img.shields.io/badge/Future-607D8B?style=flat-square&logo=futuristic&logoColor=white"/>

### Technical Improvement Roadmap 🚀
1. 📝 **Prompt optimization**
   - More precise tool usage guidance
   - Better error handling prompts
   - More efficient interaction patterns

2. 🏗️ **Architecture optimization**
   - Unified management of tool calls
   - Improved state tracking
   - Enhanced caching mechanisms

3. ⚡ **Performance optimization**
   - Reduce unnecessary interactions
   - Optimize response time
   - Improve resource utilization

---

## Conclusion <img src="https://img.shields.io/badge/Conclusion-FF5722?style=flat-square&logo=target&logoColor=white"/>

This project not only helped me deeply understand how LLMs work, but also made me realize some important principles in software engineering. Sometimes, building a reliable system isn't about making it as intelligent as possible, but about establishing the right constraints and validation mechanisms.

### Code Example 💻
```python
import json
import logging
from typing import Dict, Any, Optional, Tuple
from fastapi import UploadFile

logger = logging.getLogger(__name__)

class ToolCallController:
    """A reliable tool call controller specifically for handling LLM tool calling processes.
    
    The main responsibilities of this controller are:
    1. Manage the entire lifecycle of tool calls
    2. Ensure tools are executed correctly
    3. Verify tool references in final results
    """
    
    def __init__(self):
        # Save current tool execution results for later verification
        self.current_tool_result = None
        # Store conversation parts for management and backtracking
        self.conversation_parts = []
        # Current image file being processed
        self.current_image_file = None
        
        # Control parameters
        self.max_steps = 10  # Maximum conversation rounds
        self.early_stop_threshold = 2  # Early stopping threshold

    async def process(self, prompt: str, image_file: UploadFile) -> Dict[str, Any]:
        """Process the complete tool calling workflow
        
        Args:
            prompt: Initial prompt
            image_file: Image file to process
            
        Returns:
            Dict[str, Any]: Processing result
        """
        try:
            # Initialize state
            self.current_image_file = image_file
            self.conversation_parts = []
            self.current_tool_result = None
            
            # Start conversation loop
            conversation = prompt
            step_count = 0
            no_tool_call_count = 0
            
            while step_count < self.max_steps:
                # Get LLM response
                response = await self._get_llm_response(conversation)
                logger.info(f"LLM response: {response[:100]}...")  # Only log first 100 characters to avoid long logs
                
                # Parse tool call
                tool_name, tool_args, current_text = self._parse_latest_plugin_call(response)
                
                if tool_name:
                    # Reset no tool call count
                    no_tool_call_count = 0
                    
                    # Execute tool call
                    tool_result = await self._execute_tool(tool_name, tool_args)
                    logger.info(f"Tool return result: {tool_result}")
                    
                    # Update conversation
                    conversation = conversation + "\n" + current_text + f"\nObservation: {tool_result}\nThought:"
                else:
                    no_tool_call_count += 1
                    logger.info(f"No tool call detected in this round, count: {no_tool_call_count}")
                    
                    # Check for final answer
                    if "Final Answer:" in response:
                        # Validate and return result
                        return self._validate_result(response)
                
                # Check if early stopping is possible
                if no_tool_call_count >= self.early_stop_threshold:
                    logger.info("Early stopping condition reached")
                    break
                    
                step_count += 1
            
            raise ValueError("Maximum conversation rounds limit reached")
            
        except Exception as e:
            logger.error(f"Processing failed: {str(e)}")
            return {
                "is_compliant": False,
                "reason": f"Processing failed: {str(e)}",
                "reference": "System error"
            }

    async def _execute_tool(self, tool_name: str, tool_args: Dict[str, Any]) -> str:
        """Execute tool call, ensuring results and state are correctly recorded"""
        try:
            logger.info(f"Executing tool: {tool_name}")
            logger.info(f"Tool parameters: {json.dumps(tool_args, ensure_ascii=False)}")
            
            if tool_name == "check_logo_presence":
                if not self.current_image_file:
                    raise ValueError("No image file available")
                
                # Execute logo detection
                has_logo = await check_logo_presence(self.current_image_file)
                
                # Build standard format return result
                result = {
                    "status": "success",
                    "has_logo": has_logo,
                    "logo_position": "advertising_image" if has_logo else "not_found",
                    "logo_clarity": "clear" if has_logo else "not_applicable"
                }
                
                # Save tool return result for later verification
                self.current_tool_result = result
                
                return json.dumps(result, ensure_ascii=False)
            else:
                raise ValueError(f"Unknown tool: {tool_name}")
                
        except Exception as e:
            logger.error(f"Tool execution failed: {str(e)}")
            return json.dumps({
                "status": "error",
                "error": str(e)
            })

    def _parse_latest_plugin_call(self, text: str) -> Tuple[str, Optional[Dict], str]:
        """Parse the most recent tool call using reliable string finding methods"""
        plugin_name, plugin_args = '', None
        
        # Find positions of key parts
        action_idx = text.rfind('\nAction:')
        action_input_idx = text.rfind('\nAction Input:')
        observation_idx = text.rfind('\nObservation:')
        
        if 0 <= action_idx < action_input_idx:
            # If no Observation, add one
            if observation_idx < action_input_idx:
                text = text.rstrip() + '\nObservation:'
                observation_idx = text.rfind('\nObservation:')
            
            # Extract tool name and parameters
            plugin_name = text[action_idx + len('\nAction:'): action_input_idx].strip()
            args_text = text[action_input_idx + len('\nAction Input:'): observation_idx].strip()
            
            # Parse parameters
            if args_text:
                try:
                    plugin_args = json.loads(args_text)
                except json.JSONDecodeError as e:
                    logger.error(f"Parameter parsing failed: {e}")
                    return '', None, text
            
            # Return text up to Observation
            text = text[:observation_idx]
        
        return plugin_name, plugin_args, text

    def _validate_result(self, text: str) -> Dict[str, Any]:
        """Validate and parse final result, ensuring tool results are correctly referenced"""
        try:
            # Extract Final Answer part
            final_idx = text.rfind('Final Answer:')
            if final_idx == -1:
                raise ValueError("Final Answer not found")
                
            answer_text = text[final_idx + len('Final Answer:'):].strip()
            
            # Parse JSON
            try:
                result = json.loads(answer_text)
            except json.JSONDecodeError:
                raise ValueError("Invalid JSON format")
            
            # Validate tool result references
            if self.current_tool_result:
                # Extract tool return values referenced in reason
                if "reason" in result:
                    reason = result["reason"]
                    if ("has_logo=" in reason) and ("status=" in reason):
                        # Verify referenced values match actual tool returns
                        tool_has_logo = "has_logo=true" in reason
                        if tool_has_logo != self.current_tool_result["has_logo"]:
                            logger.error("Tool result reference mismatch detected")
                            return {
                                "is_compliant": False,
                                "reason": f"Based on actual tool return results: status={self.current_tool_result['status']}, "
                                         f"has_logo={self.current_tool_result['has_logo']}, "
                                         f"logo_position={self.current_tool_result['logo_position']}, "
                                         f"logo_clarity={self.current_tool_result['logo_clarity']}",
                                "reference": "System validation failed"
                            }
                
            return result
            
        except Exception as e:
            logger.error(f"Result validation failed: {e}")
            return {
                "is_compliant": False,
                "reason": f"Result validation failed: {str(e)}",
                "reference": "System error"
            }

    @staticmethod
    def build_prompt() -> str:
        """Build tool calling prompt"""
        return """Please analyze whether the advertisement contains a health food logo.

Important rules:
1. You MUST use the check_logo_presence tool for detection
2. Do NOT make judgments or guess results on your own
3. You MUST wait for the tool to return actual results
4. You MUST accurately reference tool return values in your final answer

Format to use:
Question: Input question
Thought: Think about the next action
Action: Tool name to use
Action Input: Tool parameters
Observation: Results returned by the tool
... (above steps can be repeated)
Thought: Now I know the answer
Final Answer: Final answer (JSON format)

Start analyzing!
"""

# Usage example
async def process_image(image_file: UploadFile) -> Dict[str, Any]:
    """Complete process for image processing"""
    controller = ToolCallController()
    prompt = controller.build_prompt()
    return await controller.process(prompt, image_file)
```

# Code Design Rationale and Improvement Points

## Core Improvements

### 1. Complete State Management
- Centralized management of all states through the `ToolCallController` class
- Includes current tool results, conversation history, and image file
- Ensures system state consistency, avoiding data loss or tampering

### 2. Reliable Tool Call Parsing
- The `_parse_latest_plugin_call` method uses string finding instead of regular expressions
- Improves reliability and maintainability
- Correctly handles edge cases (such as missing Observation)

### 3. Strict Result Validation
- The `_validate_result` method performs double validation:
  - Parses result format
  - Verifies tool return value references
- Prevents LLM from arbitrarily claiming non-existent logos

### 4. Comprehensive Error Handling
- Key methods all have complete error handling mechanisms
- Detailed information recorded through the logging system
- Facilitates problem diagnosis and system maintenance

### 5. Clear Prompt Management
- Unified management through the `build_prompt` method
- Ensures LLM understands and follows tool usage rules

## Key Design Decision Analysis

### 1. Using rfind Instead of find
```python
action_idx = text.rfind('\nAction:')
```
- Purpose: Locate the most recent tool call
- Reason: LLM may call tools multiple times, only concerned with the latest call

### 2. Saving current_tool_result
```python
self.current_tool_result = result
```
- Purpose: Verify tool return values during validation phase
- Function: Prevent LLM from tampering with results

### 3. early_stop_threshold Mechanism
```python
if no_tool_call_count >= self.early_stop_threshold:
    logger.info("Early stopping condition reached")
    break
```
- Purpose: Optimize conversation rounds
- Logic: Consecutive rounds without tool calls indicate analysis may be complete

### 4. Log Optimization
```python
logger.info(f"LLM response: {response[:100]}...")
```
- Purpose: Control log file size
- Practical experience: Recording the first 100 characters is usually sufficient for debugging

## Summary
The core value of this implementation lies in:
1. Ensuring the reliability of tool calls
2. Preventing LLM from skipping actual tool calls
3. Enabling full process monitoring through comprehensive logging

---

<div align="center">
  <p><small>© 2023 LLM Tool Calling Framework | Document Version v1.0</small></p>
</div> 